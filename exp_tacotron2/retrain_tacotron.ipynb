{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain Multi-Tacotron2 trained by jvs-dataset\n",
    "### 考慮点 \n",
    "##### speaker_idを入力に含めて学習を行う。\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from logging import Logger\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from omegaconf import DictConfig\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import hydra\n",
    "from hydra.utils import to_absolute_path\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import OmegaConf\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ttslearn.contrib.multispk_util import collate_fn_ms_tacotron, setup\n",
    "# from ttslearn.util import make_non_pad_mask\n",
    "# from ttslearn.train_util import (get_epochs_with_optional_tqdm,plot_2d_feats,plot_attention,save_checkpoint,)\n",
    "\n",
    "from utils.multispk_util import collate_fn_ms_tacotron, setup\n",
    "from utils import multispk_util\n",
    "from utils.util import make_non_pad_mask, make_pad_mask, load_utt_list\n",
    "from utils.train_util import (\n",
    "    get_epochs_with_optional_tqdm,\n",
    "    plot_2d_feats,\n",
    "    plot_attention,\n",
    "    save_checkpoint,\n",
    ")\n",
    "from ttslearn.tacotron.frontend.openjtalk import sequence_to_text\n",
    "\n",
    "from utils.early_stopping import EarlyStopping\n",
    "\n",
    "logger: Logger = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spk_idの指定＋保存\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    \"data/fine-spk.npy\",\n",
    "    np.array([52], dtype=np.int64),\n",
    "    allow_pickle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config読み込みの関数を定義\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルや学習の際の調整パラメータをhydraを使用してconfigディレクトリから取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    hydra_instance = GlobalHydra.instance()\n",
    "    if not hydra_instance.is_initialized():\n",
    "        hydra_instance.clear()\n",
    "        initialize(config_path=\"conf/train_tacotron\")\n",
    "    \n",
    "    cfg = compose(config_name=\"config\")\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(config.data.train.utt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データ数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num = 100\n",
    "dev_data_num = 100\n",
    "is_early_stopping = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainデータ数を指定して.listファイルとして作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習手順定義\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    train,\n",
    "    criterions,\n",
    "    in_feats,\n",
    "    in_lens,\n",
    "    out_feats,\n",
    "    out_lens,\n",
    "    stop_flags,\n",
    "    spk_ids,\n",
    "):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Run forwaard\n",
    "    outs, outs_fine, logits, _ = model(in_feats, in_lens, out_feats, spk_ids)\n",
    "\n",
    "    # Mask (B x T x 1)\n",
    "    # 損失を求めるためpadding部分を取り除く\n",
    "    mask = make_non_pad_mask(out_lens).unsqueeze(-1).to(out_feats.device)\n",
    "    out_feats = out_feats.masked_select(mask)\n",
    "    outs = outs.masked_select(mask)\n",
    "    outs_fine = outs_fine.masked_select(mask)\n",
    "    stop_flags = stop_flags.masked_select(mask.squeeze(-1))\n",
    "    logits = logits.masked_select(mask.squeeze(-1))\n",
    "\n",
    "    # Loss\n",
    "    decoder_out_loss = criterions[\"out_loss\"](outs, out_feats)\n",
    "    postnet_out_loss = criterions[\"out_loss\"](outs_fine, out_feats)\n",
    "    stop_token_loss = criterions[\"stop_token_loss\"](logits, stop_flags)\n",
    "    loss = decoder_out_loss + postnet_out_loss + stop_token_loss\n",
    "\n",
    "    loss_values = {\n",
    "        \"DecoderOutLoss\": decoder_out_loss.item(),\n",
    "        \"PostnetOutLoss\": postnet_out_loss.item(),\n",
    "        \"StopTokenLoss\": stop_token_loss.item(),\n",
    "        \"Loss\": loss.item(),\n",
    "    }\n",
    "\n",
    "    # Update\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        if not torch.isfinite(grad_norm):\n",
    "            logger.info(\"grad norm is NaN. Skip updating\")\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデル評価関数の定義\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(\n",
    "    step, model, writer, in_feats, in_lens, out_feats, out_lens, spk_ids, is_inference\n",
    "):\n",
    "    # 最大3つまで\n",
    "    N = min(len(in_feats), 3)\n",
    "\n",
    "    if is_inference:\n",
    "        outs, outs_fine, att_ws, out_lens = [], [], [], []\n",
    "        for idx in range(N):\n",
    "            out, out_fine, _, att_w = model.inference(\n",
    "                in_feats[idx][: in_lens[idx]], spk_ids[idx]\n",
    "            )\n",
    "            outs.append(out)\n",
    "            outs_fine.append(out_fine)\n",
    "            att_ws.append(att_w)\n",
    "            out_lens.append(len(out))\n",
    "    else:\n",
    "        outs, outs_fine, _, att_ws = model(in_feats, in_lens, out_feats, spk_ids)\n",
    "\n",
    "    for idx in range(N):\n",
    "        text = \"\".join(\n",
    "            sequence_to_text(in_feats[idx][: in_lens[idx]].cpu().data.numpy())\n",
    "        )\n",
    "        if is_inference:\n",
    "            group = f\"utt{idx+1}_inference\"\n",
    "        else:\n",
    "            group = f\"utt{idx+1}_teacher_forcing\"\n",
    "\n",
    "        out = outs[idx][: out_lens[idx]]\n",
    "        out_fine = outs_fine[idx][: out_lens[idx]]\n",
    "        rf = model.decoder.reduction_factor\n",
    "        att_w = att_ws[idx][: out_lens[idx] // rf, : in_lens[idx]]\n",
    "        fig = plot_attention(att_w)\n",
    "        writer.add_figure(f\"{group}/attention\", fig, step)\n",
    "        plt.close()\n",
    "        fig = plot_2d_feats(out, text)\n",
    "        writer.add_figure(f\"{group}/out_before_postnet\", fig, step)\n",
    "        plt.close()\n",
    "        fig = plot_2d_feats(out_fine, text)\n",
    "        writer.add_figure(f\"{group}/out_after_postnet\", fig, step)\n",
    "        plt.close()\n",
    "        if not is_inference:\n",
    "            out_gt = out_feats[idx][: out_lens[idx]]\n",
    "            fig = plot_2d_feats(out_gt, text)\n",
    "            writer.add_figure(f\"{group}/out_ground_truth\", fig, step)\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習ループ処理定義\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    nepochs loop ----->\n",
    "    ・loss_params (intervalごとにparamsを保存)\n",
    "\n",
    "        train & dev loop --->\n",
    "        ・ ave_loss (Epoch ごとのロスを記録) <- from running_loss\n",
    "        ・ best_loss_params (最小のlossの更新ごとに保存) if train\n",
    "    \n",
    "            batch loop ---------->\n",
    "            ・sort\n",
    "            ・train_step (損失を元にparamの更新)\n",
    "            ・eval_model (intervalごとに) if eval and first data\n",
    "        \n",
    "    ・latest_loss_param (保存)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 保存場所\n",
    "- writer : tensorboard/exp\n",
    "- save_checkpoint : exp\n",
    "- logger : shellログ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_running_losses_(running_losses, loss_values):\n",
    "    for key, val in loss_values.items():\n",
    "        try:\n",
    "            running_losses[key] += val\n",
    "        except KeyError:\n",
    "            running_losses[key] = val\n",
    "\n",
    "\n",
    "def train_loop(config, device, model, optimizer, lr_scheduler, data_loaders, writer, is_early_stopping):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    print(\" Eearly Stopping Mode is : \"+ str(is_early_stopping))\n",
    "    es_flag = False\n",
    "\n",
    "    criterions = {\n",
    "        \"out_loss\": nn.MSELoss(),\n",
    "        \"stop_token_loss\": nn.BCEWithLogitsLoss(),\n",
    "    }\n",
    "\n",
    "    out_dir = Path(to_absolute_path(config.train.out_dir))\n",
    "    best_loss = torch.finfo(torch.float32).max\n",
    "    train_iter = 1\n",
    "    nepochs = config.train.nepochs\n",
    "    print(\"nepochs : \" + str(nepochs))\n",
    "    print(\"iter per epochs: \" + str(train_data_num / 32))\n",
    "    print(\"itar sum : \" + str(config.train.max_train_steps))\n",
    "    # for epoch in get_epochs_with_optional_tqdm(config.tqdm, nepochs):\n",
    "    for epoch in range(1, nepochs + 1):\n",
    "        for phase in data_loaders.keys():\n",
    "            train = phase.startswith(\"train\")\n",
    "            model.train() if train else model.eval()\n",
    "            running_losses = {}\n",
    "            for idx, (\n",
    "                in_feats,\n",
    "                in_lens,\n",
    "                out_feats,\n",
    "                out_lens,\n",
    "                stop_flags,\n",
    "                spk_ids,\n",
    "            ) in tqdm(\n",
    "                enumerate(data_loaders[phase]), desc=f\"{phase} iter\"\n",
    "            ):\n",
    "                # ミニバッチのソート (短い順)\n",
    "                in_lens, indices = torch.sort(in_lens, dim=0, descending=True)\n",
    "                in_feats, out_feats, out_lens = (\n",
    "                    in_feats[indices].to(device),\n",
    "                    out_feats[indices].to(device),\n",
    "                    out_lens[indices].to(device),\n",
    "                )\n",
    "                stop_flags = stop_flags[indices].to(device)\n",
    "                spk_ids = spk_ids[indices].to(device)\n",
    "\n",
    "                loss_values = train_step(\n",
    "                    model,\n",
    "                    optimizer,\n",
    "                    lr_scheduler,\n",
    "                    train,\n",
    "                    criterions,\n",
    "                    in_feats,\n",
    "                    in_lens,\n",
    "                    out_feats,\n",
    "                    out_lens,\n",
    "                    stop_flags,\n",
    "                    spk_ids,\n",
    "                )\n",
    "\n",
    "                # memo each loss of tacotron2 & lr per batch iter\n",
    "                if train:\n",
    "                    for key, val in loss_values.items():\n",
    "                        writer.add_scalar(f\"{key}ByStep/train\", val, train_iter)\n",
    "                    writer.add_scalar(\n",
    "                        \"LearningRate\", lr_scheduler.get_last_lr()[0], train_iter\n",
    "                    )\n",
    "                    train_iter += 1\n",
    "                _update_running_losses_(running_losses, loss_values)\n",
    "\n",
    "                # 最初の検証用データに対して、中間結果の可視化\n",
    "                if (\n",
    "                    not train\n",
    "                    and idx == 0\n",
    "                    and epoch % config.train.eval_epoch_interval == 0\n",
    "                ):\n",
    "                    for is_inference in [False, True]:\n",
    "                        eval_model(\n",
    "                            train_iter,\n",
    "                            model,\n",
    "                            writer,\n",
    "                            in_feats,\n",
    "                            in_lens,\n",
    "                            out_feats,\n",
    "                            out_lens,\n",
    "                            spk_ids,\n",
    "                            is_inference,\n",
    "                        )\n",
    "\n",
    "            # Epoch ごとのロスを出力\n",
    "            for key, val in running_losses.items():\n",
    "                ave_loss = val / len(data_loaders[phase])\n",
    "                writer.add_scalar(f\"{key}/{phase}\", ave_loss, epoch)\n",
    "\n",
    "            ave_loss = running_losses[\"Loss\"] / len(data_loaders[phase])\n",
    "            if not train:\n",
    "                # 早期終了をするかのフラッグを立てる。epoch loopでbreakをかける\n",
    "                if is_early_stopping:\n",
    "                    es_flag = early_stopping(ave_loss)\n",
    "\n",
    "                if ave_loss < best_loss:\n",
    "                    best_loss = ave_loss\n",
    "                    save_checkpoint(logger, out_dir, model, optimizer, epoch, True)\n",
    "\n",
    "        if epoch % config.train.checkpoint_epoch_interval == 0:\n",
    "            save_checkpoint(logger, out_dir, model, optimizer, epoch, False)\n",
    "\n",
    "        if es_flag:\n",
    "            break\n",
    "\n",
    "    # save at last epoch\n",
    "    save_checkpoint(logger, out_dir, model, optimizer, nepochs)\n",
    "    logger.info(f\"The best loss was {best_loss}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行処理\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def choose_train_list(config, num):\n",
    "    utt_ids = load_utt_list(to_absolute_path(config.data.train.utt_list))\n",
    "    resampled = random.sample(utt_ids, num)\n",
    "    with open(f\"./data/train_{num}.list\", \"w\") as list:\n",
    "        for i in resampled:\n",
    "            list.write(str(i) + \"\\n\")\n",
    "    config.data.train.utt_list = f\"./data/train_{num}.list\"\n",
    "\n",
    "def choose_dev_list(config, num):\n",
    "    utt_ids = load_utt_list(to_absolute_path(config.data.dev.utt_list))\n",
    "    resampled = random.sample(utt_ids, num)\n",
    "    with open(f\"./data/dev_{num}.list\", \"w\") as list:\n",
    "        for i in resampled:\n",
    "            list.write(str(i) + \"\\n\")\n",
    "    config.data.dev.utt_list = f\"./data/dev_{num}.list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_app():\n",
    "    global logger\n",
    "    config = load_config()\n",
    "\n",
    "    choose_train_list(config, train_data_num)\n",
    "    choose_dev_list(config, dev_data_num)\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    collate_fn = partial(\n",
    "        collate_fn_ms_tacotron, reduction_factor=config.model.netG.reduction_factor # configはconf/train_tacotron内のすべてのファイルでのyamlが一つのconfigとして読み込まれる\n",
    "    )\n",
    "\n",
    "    model, optimizer, lr_scheduler, data_loaders, writer, logger = multispk_util.setup(\n",
    "        config, device, collate_fn\n",
    "    )\n",
    "    print(config.train.pretrained.checkpoint)\n",
    "    train_loop(config, device, model, optimizer, lr_scheduler, data_loaders, writer, is_early_stopping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/My Document/Tacotron_Exp/.taco_env/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/multspk_tacotron2_hifipwg_jvs24k/acoustic_model.pth\n",
      " Eearly Stopping Mode is : False\n",
      "nepochs : 250\n",
      "iter per epochs: 3.125\n",
      "itar sum : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train iter: 4it [02:06, 31.61s/it]\n",
      "dev iter: 4it [00:27,  6.79s/it]\n",
      "train iter: 4it [01:48, 27.09s/it]\n",
      "dev iter: 4it [00:29,  7.32s/it]\n",
      "train iter: 4it [01:40, 25.05s/it]\n",
      "dev iter: 4it [00:25,  6.33s/it]\n",
      "train iter: 4it [01:36, 24.11s/it]\n",
      "dev iter: 4it [00:25,  6.31s/it]\n",
      "train iter: 4it [01:28, 22.18s/it]\n",
      "dev iter: 4it [00:26,  6.67s/it]\n",
      "train iter: 4it [02:16, 34.06s/it]\n",
      "dev iter: 4it [00:26,  6.72s/it]\n",
      "train iter: 4it [01:34, 23.60s/it]\n",
      "dev iter: 4it [00:27,  6.82s/it]\n",
      "train iter: 4it [02:08, 32.13s/it]\n",
      "dev iter: 4it [10:06, 151.69s/it]\n",
      "train iter: 4it [01:44, 26.06s/it]\n",
      "dev iter: 4it [00:25,  6.29s/it]\n",
      "train iter: 4it [01:32, 23.01s/it]\n",
      "dev iter: 4it [00:25,  6.32s/it]\n",
      "train iter: 4it [2:32:05, 2281.29s/it]\n",
      "dev iter: 4it [15:26, 231.57s/it]\n",
      "train iter: 4it [1:00:49, 912.27s/it]\n",
      "dev iter: 4it [14:39, 219.83s/it]\n",
      "train iter: 4it [1:34:18, 1414.67s/it]\n",
      "dev iter: 4it [57:58, 869.58s/it] \n",
      "train iter: 4it [03:57, 59.40s/it] \n",
      "dev iter: 4it [00:24,  6.21s/it]\n",
      "train iter: 4it [05:41, 85.28s/it] \n",
      "dev iter: 4it [00:25,  6.36s/it]\n",
      "train iter: 4it [01:39, 24.95s/it]\n",
      "dev iter: 4it [00:24,  6.24s/it]\n",
      "train iter: 4it [01:36, 24.20s/it]\n",
      "dev iter: 4it [00:25,  6.39s/it]\n",
      "train iter: 4it [01:44, 26.03s/it]\n",
      "dev iter: 4it [00:30,  7.64s/it]\n",
      "train iter: 4it [01:33, 23.30s/it]\n",
      "dev iter: 4it [00:24,  6.15s/it]\n",
      "train iter: 4it [01:46, 26.67s/it]\n",
      "dev iter: 4it [00:29,  7.32s/it]\n",
      "train iter: 4it [01:47, 26.79s/it]\n",
      "dev iter: 4it [00:27,  6.90s/it]\n",
      "train iter: 4it [01:42, 25.74s/it]\n",
      "dev iter: 4it [00:31,  7.89s/it]\n",
      "train iter: 4it [01:37, 24.50s/it]\n",
      "dev iter: 4it [00:25,  6.37s/it]\n",
      "train iter: 4it [01:36, 24.01s/it]\n",
      "dev iter: 4it [00:26,  6.63s/it]\n",
      "train iter: 4it [01:37, 24.31s/it]\n",
      "dev iter: 4it [00:26,  6.73s/it]\n",
      "train iter: 4it [02:23, 35.90s/it]\n",
      "dev iter: 4it [00:37,  9.27s/it]\n",
      "train iter: 4it [02:05, 31.37s/it]\n",
      "dev iter: 4it [00:31,  7.87s/it]\n",
      "train iter: 4it [04:00, 60.08s/it] \n",
      "dev iter: 4it [00:37,  9.41s/it]\n",
      "train iter: 4it [03:07, 46.90s/it]\n",
      "dev iter: 4it [00:34,  8.65s/it]\n",
      "train iter: 4it [03:11, 47.82s/it]\n",
      "dev iter: 4it [00:51, 12.77s/it]\n",
      "train iter: 4it [03:11, 47.87s/it]\n",
      "dev iter: 4it [00:38,  9.57s/it]\n",
      "train iter: 4it [04:03, 60.90s/it]\n",
      "dev iter: 4it [00:34,  8.72s/it]\n",
      "train iter: 4it [03:33, 53.40s/it]\n",
      "dev iter: 4it [00:38,  9.63s/it]\n",
      "train iter: 4it [04:51, 72.96s/it]\n",
      "dev iter: 4it [00:41, 10.32s/it]\n",
      "train iter: 4it [03:19, 49.87s/it]\n",
      "dev iter: 4it [00:47, 11.98s/it]\n",
      "train iter: 4it [03:56, 59.05s/it]\n",
      "dev iter: 4it [00:40, 10.01s/it]\n",
      "train iter: 4it [03:37, 54.41s/it]\n",
      "dev iter: 4it [00:43, 10.85s/it]\n",
      "train iter: 4it [03:15, 48.97s/it]\n",
      "dev iter: 4it [00:34,  8.57s/it]\n",
      "train iter: 4it [02:46, 41.51s/it]\n",
      "dev iter: 4it [00:33,  8.36s/it]\n",
      "train iter: 4it [02:48, 42.12s/it]\n",
      "dev iter: 4it [00:31,  7.90s/it]\n",
      "train iter: 4it [02:47, 41.78s/it]\n",
      "dev iter: 4it [00:32,  8.20s/it]\n",
      "train iter: 4it [02:57, 44.37s/it]\n",
      "dev iter: 4it [00:40, 10.20s/it]\n",
      "train iter: 4it [04:12, 63.05s/it]\n",
      "dev iter: 4it [00:40, 10.03s/it]\n",
      "train iter: 4it [02:59, 44.97s/it]\n",
      "dev iter: 4it [00:33,  8.49s/it]\n",
      "train iter: 4it [26:01, 390.42s/it]\n",
      "dev iter: 4it [00:50, 12.69s/it]\n",
      "train iter: 4it [03:43, 55.86s/it]\n",
      "dev iter: 4it [00:49, 12.42s/it]\n",
      "train iter: 4it [03:41, 55.36s/it]\n",
      "dev iter: 4it [00:42, 10.51s/it]\n",
      "train iter: 4it [03:25, 51.30s/it]\n",
      "dev iter: 4it [00:36,  9.07s/it]\n",
      "train iter: 4it [03:37, 54.28s/it]\n",
      "dev iter: 4it [00:38,  9.69s/it]\n",
      "train iter: 4it [03:41, 55.45s/it]\n",
      "dev iter: 4it [00:36,  9.07s/it]\n",
      "train iter: 4it [03:09, 47.45s/it]\n",
      "dev iter: 4it [00:35,  8.86s/it]\n",
      "train iter: 4it [03:01, 45.33s/it]\n",
      "dev iter: 4it [00:33,  8.43s/it]\n",
      "train iter: 4it [03:12, 48.18s/it]\n",
      "dev iter: 4it [00:36,  9.16s/it]\n",
      "train iter: 4it [03:02, 45.70s/it]\n",
      "dev iter: 4it [00:33,  8.35s/it]\n",
      "train iter: 4it [03:11, 47.90s/it]\n",
      "dev iter: 4it [00:35,  8.86s/it]\n",
      "train iter: 4it [03:29, 52.33s/it]\n",
      "dev iter: 4it [00:37,  9.37s/it]\n",
      "train iter: 4it [03:28, 52.02s/it]\n",
      "dev iter: 4it [00:39,  9.97s/it]\n",
      "train iter: 4it [02:59, 44.82s/it]\n",
      "dev iter: 4it [00:35,  8.96s/it]\n",
      "train iter: 4it [03:36, 54.13s/it]\n",
      "dev iter: 4it [00:38,  9.60s/it]\n",
      "train iter: 4it [04:34, 68.61s/it]\n",
      "dev iter: 4it [00:55, 13.89s/it]\n",
      "train iter: 4it [03:46, 56.73s/it]\n",
      "dev iter: 4it [00:40, 10.21s/it]\n",
      "train iter: 4it [04:24, 66.06s/it]\n",
      "dev iter: 4it [00:39,  9.93s/it]\n",
      "train iter: 4it [03:10, 47.62s/it]\n",
      "dev iter: 4it [00:38,  9.58s/it]\n",
      "train iter: 4it [03:40, 55.03s/it]\n",
      "dev iter: 4it [00:42, 10.52s/it]\n",
      "train iter: 4it [03:22, 50.64s/it]\n",
      "dev iter: 4it [01:01, 15.44s/it]\n",
      "train iter: 4it [05:40, 85.14s/it] \n",
      "dev iter: 4it [00:42, 10.59s/it]\n",
      "train iter: 4it [03:42, 55.66s/it]\n",
      "dev iter: 4it [00:40, 10.11s/it]\n",
      "train iter: 4it [03:32, 53.23s/it]\n",
      "dev iter: 4it [00:44, 11.02s/it]\n",
      "train iter: 4it [03:36, 54.03s/it]\n",
      "dev iter: 4it [00:40, 10.08s/it]\n",
      "train iter: 4it [03:46, 56.56s/it]\n",
      "dev iter: 4it [00:40, 10.24s/it]\n",
      "train iter: 4it [03:16, 49.22s/it]\n",
      "dev iter: 4it [00:38,  9.66s/it]\n",
      "train iter: 4it [04:20, 65.04s/it]\n",
      "dev iter: 4it [00:39,  9.77s/it]\n",
      "train iter: 4it [04:37, 69.34s/it]\n",
      "dev iter: 4it [00:48, 12.24s/it]\n",
      "train iter: 4it [03:55, 58.78s/it]\n",
      "dev iter: 4it [00:40, 10.13s/it]\n",
      "train iter: 4it [03:29, 52.37s/it]\n",
      "dev iter: 4it [00:39,  9.78s/it]\n",
      "train iter: 4it [03:38, 54.54s/it]\n",
      "dev iter: 4it [00:39,  9.86s/it]\n",
      "train iter: 4it [03:50, 57.73s/it]\n",
      "dev iter: 4it [00:38,  9.63s/it]\n",
      "train iter: 4it [03:28, 52.17s/it]\n",
      "dev iter: 4it [00:38,  9.63s/it]\n",
      "train iter: 4it [03:30, 52.66s/it]\n",
      "dev iter: 4it [00:39,  9.80s/it]\n",
      "train iter: 4it [03:49, 57.27s/it]\n",
      "dev iter: 4it [00:41, 10.31s/it]\n",
      "train iter: 4it [03:52, 58.03s/it]\n",
      "dev iter: 4it [00:39,  9.90s/it]\n",
      "train iter: 4it [03:38, 54.57s/it]\n",
      "dev iter: 4it [00:39,  9.95s/it]\n",
      "train iter: 4it [03:42, 55.59s/it]\n",
      "dev iter: 4it [00:39,  9.77s/it]\n",
      "train iter: 4it [03:29, 52.40s/it]\n",
      "dev iter: 4it [00:38,  9.63s/it]\n",
      "train iter: 4it [03:32, 53.13s/it]\n",
      "dev iter: 4it [00:38,  9.69s/it]\n",
      "train iter: 4it [03:25, 51.35s/it]\n",
      "dev iter: 4it [00:39,  9.90s/it]\n",
      "train iter: 4it [03:41, 55.46s/it]\n",
      "dev iter: 4it [00:41, 10.40s/it]\n",
      "train iter: 4it [03:37, 54.35s/it]\n",
      "dev iter: 4it [00:40, 10.23s/it]\n",
      "train iter: 4it [03:30, 52.51s/it]\n",
      "dev iter: 4it [00:39,  9.88s/it]\n",
      "train iter: 4it [04:12, 63.10s/it]\n",
      "dev iter: 4it [00:39,  9.91s/it]\n",
      "train iter: 4it [03:32, 53.06s/it]\n",
      "dev iter: 4it [00:38,  9.65s/it]\n",
      "train iter: 4it [03:45, 56.41s/it]\n",
      "dev iter: 4it [00:40, 10.06s/it]\n",
      "train iter: 4it [03:40, 55.01s/it]\n",
      "dev iter: 4it [00:38,  9.64s/it]\n",
      "train iter: 4it [03:59, 59.75s/it]\n",
      "dev iter: 4it [00:43, 10.89s/it]\n",
      "train iter: 4it [04:02, 60.69s/it]\n",
      "dev iter: 4it [00:40, 10.20s/it]\n",
      "train iter: 4it [03:53, 58.37s/it]\n",
      "dev iter: 4it [00:42, 10.69s/it]\n",
      "train iter: 4it [04:37, 69.25s/it]\n",
      "dev iter: 4it [00:41, 10.27s/it]\n",
      "train iter: 4it [03:54, 58.61s/it]\n",
      "dev iter: 4it [00:39,  9.90s/it]\n",
      "train iter: 4it [03:48, 57.14s/it]\n",
      "dev iter: 4it [00:40, 10.19s/it]\n",
      "train iter: 4it [03:51, 57.79s/it]\n",
      "dev iter: 4it [01:47, 26.88s/it]\n",
      "train iter: 4it [03:54, 58.68s/it]\n",
      "dev iter: 4it [00:39,  9.83s/it]\n",
      "train iter: 4it [04:00, 60.14s/it]\n",
      "dev iter: 4it [00:41, 10.31s/it]\n",
      "train iter: 4it [03:54, 58.57s/it]\n",
      "dev iter: 4it [00:40, 10.11s/it]\n",
      "train iter: 4it [03:30, 52.66s/it]\n",
      "dev iter: 4it [00:39,  9.99s/it]\n",
      "train iter: 4it [03:51, 57.88s/it]\n",
      "dev iter: 4it [00:39,  9.94s/it]\n",
      "train iter: 4it [04:15, 64.00s/it]\n",
      "dev iter: 4it [00:39,  9.94s/it]\n",
      "train iter: 4it [04:10, 62.66s/it]\n",
      "dev iter: 4it [00:42, 10.54s/it]\n",
      "train iter: 4it [04:14, 63.64s/it]\n",
      "dev iter: 4it [00:39,  9.87s/it]\n",
      "train iter: 4it [03:39, 54.76s/it]\n",
      "dev iter: 4it [00:39,  9.78s/it]\n",
      "train iter: 4it [03:54, 58.58s/it]\n",
      "dev iter: 4it [00:40, 10.01s/it]\n",
      "train iter: 4it [03:32, 53.13s/it]\n",
      "dev iter: 4it [00:42, 10.67s/it]\n",
      "train iter: 4it [03:39, 54.95s/it]\n",
      "dev iter: 4it [00:41, 10.37s/it]\n",
      "train iter: 4it [03:51, 57.88s/it]\n",
      "dev iter: 4it [00:42, 10.63s/it]\n",
      "train iter: 4it [03:52, 58.12s/it]\n",
      "dev iter: 4it [00:41, 10.49s/it]\n",
      "train iter: 4it [03:58, 59.53s/it]\n",
      "dev iter: 4it [00:40, 10.01s/it]\n",
      "train iter: 4it [04:01, 60.34s/it]\n",
      "dev iter: 4it [00:40, 10.16s/it]\n",
      "train iter: 4it [03:56, 59.11s/it]\n",
      "dev iter: 4it [00:39,  9.97s/it]\n",
      "train iter: 4it [04:20, 65.10s/it]\n",
      "dev iter: 4it [00:41, 10.38s/it]\n",
      "train iter: 4it [04:18, 64.71s/it]\n",
      "dev iter: 4it [00:40, 10.21s/it]\n",
      "train iter: 4it [04:10, 62.50s/it]\n",
      "dev iter: 4it [00:43, 10.94s/it]\n",
      "train iter: 4it [04:08, 62.24s/it]\n",
      "dev iter: 4it [00:44, 11.21s/it]\n",
      "train iter: 4it [03:53, 58.44s/it]\n",
      "dev iter: 4it [00:39,  9.93s/it]\n",
      "train iter: 4it [03:45, 56.32s/it]\n",
      "dev iter: 4it [00:40, 10.19s/it]\n",
      "train iter: 4it [04:19, 64.87s/it]\n",
      "dev iter: 4it [00:42, 10.52s/it]\n",
      "train iter: 4it [04:05, 61.32s/it]\n",
      "dev iter: 4it [00:43, 10.76s/it]\n",
      "train iter: 4it [04:25, 66.35s/it]\n",
      "dev iter: 4it [00:40, 10.10s/it]\n",
      "train iter: 4it [04:07, 61.90s/it]\n",
      "dev iter: 4it [00:39,  9.90s/it]\n",
      "train iter: 4it [04:08, 62.21s/it]\n",
      "dev iter: 4it [00:39,  9.86s/it]\n",
      "train iter: 4it [03:45, 56.39s/it]\n",
      "dev iter: 4it [00:39,  9.84s/it]\n",
      "train iter: 4it [03:47, 56.91s/it]\n",
      "dev iter: 4it [00:44, 11.16s/it]\n",
      "train iter: 4it [03:57, 59.42s/it]\n",
      "dev iter: 4it [00:43, 10.98s/it]\n",
      "train iter: 4it [04:07, 61.94s/it]\n",
      "dev iter: 4it [00:40, 10.22s/it]\n",
      "train iter: 4it [04:26, 66.69s/it]\n",
      "dev iter: 4it [00:40, 10.23s/it]\n",
      "train iter: 4it [04:17, 64.37s/it]\n",
      "dev iter: 4it [00:43, 10.85s/it]\n",
      "train iter: 4it [03:29, 52.25s/it]\n",
      "dev iter: 4it [00:39,  9.79s/it]\n",
      "train iter: 4it [03:54, 58.65s/it]\n",
      "dev iter: 4it [00:41, 10.31s/it]\n",
      "train iter: 4it [03:52, 58.18s/it]\n",
      "dev iter: 4it [00:42, 10.68s/it]\n",
      "train iter: 4it [04:33, 68.44s/it]\n",
      "dev iter: 4it [00:40, 10.11s/it]\n",
      "train iter: 4it [04:33, 68.26s/it]\n",
      "dev iter: 4it [00:41, 10.28s/it]\n",
      "train iter: 4it [04:30, 67.67s/it]\n",
      "dev iter: 4it [00:39,  9.85s/it]\n",
      "train iter: 4it [05:01, 75.31s/it]\n",
      "dev iter: 4it [00:41, 10.48s/it]\n",
      "train iter: 4it [04:25, 66.42s/it]\n",
      "dev iter: 4it [00:42, 10.60s/it]\n",
      "train iter: 4it [05:04, 76.20s/it]\n",
      "dev iter: 4it [00:39,  9.98s/it]\n",
      "train iter: 4it [04:39, 69.89s/it]\n",
      "dev iter: 4it [00:41, 10.32s/it]\n",
      "train iter: 4it [04:17, 64.47s/it]\n",
      "dev iter: 4it [00:42, 10.52s/it]\n",
      "train iter: 4it [03:51, 58.00s/it]\n",
      "dev iter: 4it [00:41, 10.39s/it]\n",
      "train iter: 4it [04:12, 63.01s/it]\n",
      "dev iter: 4it [00:40, 10.17s/it]\n",
      "train iter: 4it [03:56, 59.01s/it]\n",
      "dev iter: 4it [00:39,  9.96s/it]\n",
      "train iter: 4it [03:54, 58.71s/it]\n",
      "dev iter: 4it [00:42, 10.64s/it]\n",
      "train iter: 4it [04:31, 67.78s/it]\n",
      "dev iter: 4it [00:40, 10.20s/it]\n",
      "train iter: 4it [03:58, 59.56s/it]\n",
      "dev iter: 4it [00:40, 10.22s/it]\n",
      "train iter: 4it [04:04, 61.17s/it]\n",
      "dev iter: 4it [00:39,  9.86s/it]\n",
      "train iter: 4it [03:57, 59.50s/it]\n",
      "dev iter: 4it [00:40, 10.21s/it]\n",
      "train iter: 4it [04:12, 63.08s/it]\n",
      "dev iter: 4it [00:41, 10.36s/it]\n",
      "train iter: 4it [04:09, 62.28s/it]\n",
      "dev iter: 4it [00:43, 10.83s/it]\n",
      "train iter: 4it [04:18, 64.70s/it]\n",
      "dev iter: 4it [00:44, 11.22s/it]\n",
      "train iter: 4it [04:17, 64.27s/it]\n",
      "dev iter: 4it [00:42, 10.63s/it]\n",
      "train iter: 4it [04:32, 68.06s/it]\n",
      "dev iter: 4it [00:43, 10.89s/it]\n",
      "train iter: 4it [04:57, 74.47s/it]\n",
      "dev iter: 4it [00:41, 10.42s/it]\n",
      "train iter: 4it [04:40, 70.10s/it]\n",
      "dev iter: 4it [00:41, 10.25s/it]\n",
      "train iter: 4it [04:21, 65.41s/it]\n",
      "dev iter: 4it [00:42, 10.52s/it]\n",
      "train iter: 4it [04:59, 74.90s/it]\n",
      "dev iter: 4it [00:42, 10.65s/it]\n",
      "train iter: 4it [04:06, 61.59s/it]\n",
      "dev iter: 4it [00:41, 10.42s/it]\n",
      "train iter: 4it [04:57, 74.44s/it]\n",
      "dev iter: 4it [00:39,  9.97s/it]\n",
      "train iter: 4it [04:17, 64.43s/it]\n",
      "dev iter: 4it [00:44, 11.23s/it]\n",
      "train iter: 4it [04:36, 69.22s/it]\n",
      "dev iter: 4it [00:42, 10.63s/it]\n",
      "train iter: 4it [06:47, 101.76s/it]\n",
      "dev iter: 4it [00:44, 11.06s/it]\n",
      "train iter: 4it [06:37, 99.32s/it] \n",
      "dev iter: 4it [00:44, 11.01s/it]\n",
      "train iter: 4it [04:58, 74.68s/it]\n",
      "dev iter: 4it [00:41, 10.37s/it]\n",
      "train iter: 4it [06:55, 103.75s/it]\n",
      "dev iter: 4it [00:42, 10.54s/it]\n",
      "train iter: 4it [04:45, 71.38s/it]\n",
      "dev iter: 4it [00:42, 10.73s/it]\n",
      "train iter: 4it [04:58, 74.52s/it]\n",
      "dev iter: 4it [00:46, 11.68s/it]\n",
      "train iter: 4it [05:13, 78.47s/it]\n",
      "dev iter: 4it [00:41, 10.36s/it]\n",
      "train iter: 4it [04:00, 60.24s/it]\n",
      "dev iter: 4it [00:44, 11.16s/it]\n",
      "train iter: 4it [05:12, 78.06s/it] \n",
      "dev iter: 4it [00:42, 10.71s/it]\n",
      "train iter: 4it [04:19, 64.94s/it]\n",
      "dev iter: 4it [00:40, 10.13s/it]\n",
      "train iter: 4it [04:41, 70.35s/it]\n",
      "dev iter: 4it [00:42, 10.69s/it]\n",
      "train iter: 4it [04:53, 73.37s/it]\n",
      "dev iter: 4it [00:47, 11.84s/it]\n",
      "train iter: 4it [04:59, 74.90s/it] \n",
      "dev iter: 4it [00:42, 10.64s/it]\n",
      "train iter: 4it [04:36, 69.23s/it]\n",
      "dev iter: 4it [00:42, 10.56s/it]\n",
      "train iter: 4it [05:28, 82.23s/it]\n",
      "dev iter: 4it [00:48, 12.20s/it]\n",
      "train iter: 4it [05:43, 85.86s/it] \n",
      "dev iter: 4it [00:48, 12.12s/it]\n",
      "train iter: 4it [04:31, 67.89s/it]\n",
      "dev iter: 4it [00:49, 12.36s/it]\n",
      "train iter: 4it [05:36, 84.20s/it]\n",
      "dev iter: 4it [00:46, 11.60s/it]\n",
      "train iter: 4it [05:43, 85.93s/it]\n",
      "dev iter: 4it [00:48, 12.12s/it]\n",
      "train iter: 4it [05:32, 83.21s/it]\n",
      "dev iter: 4it [00:45, 11.27s/it]\n",
      "train iter: 4it [10:22, 155.54s/it]\n",
      "dev iter: 4it [00:45, 11.27s/it]\n",
      "train iter: 4it [06:35, 98.96s/it] \n",
      "dev iter: 4it [00:44, 11.05s/it]\n",
      "train iter: 4it [04:01, 60.40s/it]\n",
      "dev iter: 4it [00:58, 14.72s/it]\n",
      "train iter: 4it [16:49, 252.42s/it]\n",
      "dev iter: 4it [00:46, 11.66s/it]\n",
      "train iter: 4it [04:31, 67.83s/it]\n",
      "dev iter: 4it [00:54, 13.60s/it]\n",
      "train iter: 4it [16:41, 250.46s/it]\n",
      "dev iter: 4it [00:49, 12.50s/it]\n",
      "train iter: 4it [11:05, 166.45s/it]\n",
      "dev iter: 4it [00:40, 10.19s/it]\n",
      "train iter: 4it [07:20, 110.11s/it]\n",
      "dev iter: 4it [00:38,  9.56s/it]\n",
      "train iter: 4it [04:58, 74.70s/it]\n",
      "dev iter: 4it [00:45, 11.50s/it]\n",
      "train iter: 4it [07:04, 106.22s/it]\n",
      "dev iter: 4it [00:47, 11.96s/it]\n",
      "train iter: 4it [07:08, 107.19s/it]\n",
      "dev iter: 4it [00:44, 11.10s/it]\n",
      "train iter: 4it [07:08, 107.16s/it]\n",
      "dev iter: 4it [00:44, 11.24s/it]\n",
      "train iter: 4it [09:49, 147.28s/it]\n",
      "dev iter: 4it [00:44, 11.14s/it]\n",
      "train iter: 4it [08:05, 121.29s/it]\n",
      "dev iter: 4it [03:04, 46.09s/it] \n",
      "train iter: 4it [06:11, 92.87s/it]\n",
      "dev iter: 4it [00:43, 10.76s/it]\n",
      "train iter: 4it [04:16, 64.03s/it] \n",
      "dev iter: 4it [00:43, 10.91s/it]\n",
      "train iter: 4it [04:30, 67.73s/it]\n",
      "dev iter: 4it [00:41, 10.41s/it]\n",
      "train iter: 4it [06:05, 91.29s/it] \n",
      "dev iter: 4it [00:41, 10.41s/it]\n",
      "train iter: 4it [05:06, 76.75s/it]\n",
      "dev iter: 4it [00:43, 10.82s/it]\n",
      "train iter: 4it [1:07:45, 1016.29s/it]\n",
      "dev iter: 4it [00:41, 10.34s/it]\n",
      "train iter: 4it [07:22, 110.63s/it]\n",
      "dev iter: 4it [00:41, 10.27s/it]\n",
      "train iter: 4it [12:59, 194.81s/it]\n",
      "dev iter: 4it [00:40, 10.19s/it]\n",
      "train iter: 4it [05:38, 84.65s/it] \n",
      "dev iter: 4it [00:38,  9.52s/it]\n",
      "train iter: 4it [04:46, 71.70s/it]\n",
      "dev iter: 4it [00:47, 11.77s/it]\n",
      "train iter: 4it [09:45, 146.42s/it]\n",
      "dev iter: 4it [00:37,  9.33s/it]\n",
      "train iter: 4it [10:07, 151.79s/it]\n",
      "dev iter: 4it [00:42, 10.64s/it]\n",
      "train iter: 4it [10:34, 158.64s/it]\n",
      "dev iter: 4it [00:42, 10.74s/it]\n",
      "train iter: 4it [11:25, 171.38s/it]\n",
      "dev iter: 4it [00:38,  9.64s/it]\n",
      "train iter: 4it [05:06, 76.64s/it]\n",
      "dev iter: 4it [00:40, 10.03s/it]\n",
      "train iter: 4it [04:31, 67.89s/it]\n",
      "dev iter: 4it [00:37,  9.27s/it]\n",
      "train iter: 4it [06:13, 93.46s/it] \n",
      "dev iter: 4it [00:39,  9.94s/it]\n",
      "train iter: 4it [05:13, 78.31s/it]\n",
      "dev iter: 4it [00:35,  8.76s/it]\n",
      "train iter: 4it [04:03, 60.99s/it]\n",
      "dev iter: 4it [00:35,  8.78s/it]\n",
      "train iter: 4it [06:16, 94.01s/it] \n",
      "dev iter: 4it [00:38,  9.53s/it]\n",
      "train iter: 4it [03:57, 59.36s/it]\n",
      "dev iter: 4it [00:38,  9.57s/it]\n",
      "train iter: 4it [05:25, 81.25s/it] \n",
      "dev iter: 4it [00:36,  9.05s/it]\n",
      "train iter: 4it [08:13, 123.40s/it]\n",
      "dev iter: 4it [00:43, 10.93s/it]\n",
      "train iter: 4it [14:37, 219.28s/it]\n",
      "dev iter: 4it [00:41, 10.48s/it]\n",
      "train iter: 4it [24:41, 370.41s/it]\n",
      "dev iter: 4it [00:44, 11.20s/it]\n",
      "train iter: 4it [13:57, 209.27s/it]\n",
      "dev iter: 4it [00:40, 10.22s/it]\n",
      "train iter: 4it [07:49, 117.25s/it]\n",
      "dev iter: 4it [00:40, 10.17s/it]\n",
      "train iter: 4it [07:51, 117.96s/it]\n",
      "dev iter: 4it [00:39,  9.88s/it]\n",
      "train iter: 4it [06:53, 103.42s/it]\n",
      "dev iter: 4it [00:41, 10.32s/it]\n",
      "train iter: 4it [05:32, 83.18s/it]\n",
      "dev iter: 4it [00:38,  9.67s/it]\n",
      "train iter: 4it [07:08, 107.15s/it]\n",
      "dev iter: 4it [00:38,  9.68s/it]\n",
      "train iter: 4it [11:30, 172.75s/it]\n",
      "dev iter: 4it [00:42, 10.61s/it]\n",
      "train iter: 4it [09:18, 139.52s/it]\n",
      "dev iter: 4it [00:40, 10.17s/it]\n",
      "train iter: 4it [18:37, 279.28s/it]\n",
      "dev iter: 4it [00:43, 10.78s/it]\n",
      "train iter: 4it [08:17, 124.44s/it]\n",
      "dev iter: 4it [00:41, 10.48s/it]\n",
      "train iter: 4it [21:51, 327.80s/it]\n",
      "dev iter: 4it [00:51, 12.80s/it]\n",
      "train iter: 1it [05:31, 331.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(multispk_util)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmy_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m, in \u001b[0;36mmy_app\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m model, optimizer, lr_scheduler, data_loaders, writer, logger \u001b[38;5;241m=\u001b[39m multispk_util\u001b[38;5;241m.\u001b[39msetup(\n\u001b[1;32m     15\u001b[0m     config, device, collate_fn\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mpretrained\u001b[38;5;241m.\u001b[39mcheckpoint)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_early_stopping\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 52\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(config, device, model, optimizer, lr_scheduler, data_loaders, writer, is_early_stopping)\u001b[0m\n\u001b[1;32m     49\u001b[0m stop_flags \u001b[38;5;241m=\u001b[39m stop_flags[indices]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     50\u001b[0m spk_ids \u001b[38;5;241m=\u001b[39m spk_ids[indices]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 52\u001b[0m loss_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_lens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_lens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspk_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# memo each loss of tacotron2 & lr per batch iter\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "Cell \u001b[0;32mIn[34], line 22\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, lr_scheduler, train, criterions, in_feats, in_lens, out_feats, out_lens, stop_flags, spk_ids)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Mask (B x T x 1)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 損失を求めるためpadding部分を取り除く\u001b[39;00m\n\u001b[1;32m     21\u001b[0m mask \u001b[38;5;241m=\u001b[39m make_non_pad_mask(out_lens)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(out_feats\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 22\u001b[0m out_feats \u001b[38;5;241m=\u001b[39m \u001b[43mout_feats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m outs \u001b[38;5;241m=\u001b[39m outs\u001b[38;5;241m.\u001b[39mmasked_select(mask)\n\u001b[1;32m     24\u001b[0m outs_fine \u001b[38;5;241m=\u001b[39m outs_fine\u001b[38;5;241m.\u001b[39mmasked_select(mask)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(multispk_util)\n",
    "my_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".taco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
