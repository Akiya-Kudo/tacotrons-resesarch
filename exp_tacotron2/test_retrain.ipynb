{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from logging import Logger\n",
    "from pathlib import Path\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "from hydra.utils import to_absolute_path\n",
    "from matplotlib import pyplot as plt\n",
    "from omegaconf import DictConfig\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/My Document/Tacotron_Exp/.taco_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from ttslearn.contrib.multispk_util import collate_fn_ms_tacotron, setup\n",
    "from utils.multispk_util import collate_fn_ms_tacotron, setup\n",
    "from ttslearn.tacotron.frontend.openjtalk import sequence_to_text\n",
    "from ttslearn.util import make_non_pad_mask\n",
    "from ttslearn.train_util import (\n",
    "    get_epochs_with_optional_tqdm,\n",
    "    plot_2d_feats,\n",
    "    plot_attention,\n",
    "    save_checkpoint,\n",
    ")\n",
    "\n",
    "logger: Logger = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "from hydra.utils import to_absolute_path\n",
    "from omegaconf import OmegaConf\n",
    "from torch import nn, optim\n",
    "from torch.utils import data as data_utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ttslearn.logger import getLogger\n",
    "from ttslearn.train_util import (\n",
    "    ensure_divisible_by,\n",
    "    num_trainable_params,\n",
    "    set_epochs_based_on_max_steps_,\n",
    ")\n",
    "from ttslearn.util import init_seed, load_utt_list, pad_1d, pad_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ用意テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data_utils.Dataset): \n",
    "\n",
    "    def __init__(self, in_paths, out_paths, spk_paths):\n",
    "        self.in_paths = in_paths\n",
    "        self.out_paths = out_paths\n",
    "        self.spk_paths = spk_paths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spk_id = np.load(self.spk_paths[idx])\n",
    "        return np.load(self.in_paths[idx]), np.load(self.out_paths[idx]), spk_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.in_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_ms_tacotron(batch, reduction_factor=1):\n",
    "    xs = [x[0] for x in batch]\n",
    "    ys = [ensure_divisible_by(x[1], reduction_factor) for x in batch]\n",
    "    spk_ids = torch.tensor([int(x[2]) for x in batch], dtype=torch.long).view(-1, 1)\n",
    "    in_lens = [len(x) for x in xs]\n",
    "    out_lens = [len(y) for y in ys]\n",
    "    in_max_len = max(in_lens)\n",
    "    out_max_len = max(out_lens)\n",
    "    x_batch = torch.stack([torch.from_numpy(pad_1d(x, in_max_len)) for x in xs])\n",
    "    y_batch = torch.stack([torch.from_numpy(pad_2d(y, out_max_len)) for y in ys])\n",
    "    il_batch = torch.tensor(in_lens, dtype=torch.long)\n",
    "    ol_batch = torch.tensor(out_lens, dtype=torch.long)\n",
    "    stop_flags = torch.zeros(y_batch.shape[0], y_batch.shape[1])\n",
    "    for idx, out_len in enumerate(out_lens):\n",
    "        stop_flags[idx, out_len - 1 :] = 1.0\n",
    "\n",
    "    return x_batch, il_batch, y_batch, ol_batch, stop_flags, spk_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_loaders(data_config, collate_fn):\n",
    "def get_data_loaders(collate_fn):\n",
    "    data_loaders = {}\n",
    "\n",
    "    for phase in [\"train\", \"dev\"]:\n",
    "        utt_ids = load_utt_list(to_absolute_path(f\"data/{phase}.list\"))\n",
    "        in_dir = Path(to_absolute_path(f\"dump/hfc_men_sr24000/norm/{phase}/in_tacotron\"))\n",
    "        out_dir = Path(to_absolute_path(f\"dump/hfc_men_sr24000/norm/{phase}/out_tacotron\"))\n",
    "\n",
    "        in_feats_paths = [in_dir / f\"{utt_id}-feats.npy\" for utt_id in utt_ids]\n",
    "        out_feats_paths = [out_dir / f\"{utt_id}-feats.npy\" for utt_id in utt_ids]\n",
    "        # spk_id_paths = [in_dir / f\"{utt_id}-spk.npy\" for utt_id in utt_ids]\n",
    "        spk_id_paths = [ \"data/fine-spk.npy\" for utt_id in utt_ids]\n",
    "\n",
    "        dataset = Dataset(in_feats_paths, out_feats_paths, spk_id_paths) # <- spk_idを自身で実装するため、その処理の変更を学習処理で行う\n",
    "        data_loaders[phase] = data_utils.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=32,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True,\n",
    "            num_workers=4,\n",
    "            shuffle=phase.startswith(\"train\"),\n",
    "        )\n",
    "\n",
    "    return data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save(\n",
    "    \"data/fine-spk.npy\",\n",
    "    np.array([10], dtype=np.int64),\n",
    "    allow_pickle=False,\n",
    ")\n",
    "\n",
    "collate_fn = partial(\n",
    "        collate_fn_ms_tacotron, reduction_factor=2\n",
    "    )\n",
    "data_loaders = get_data_loaders(collate_fn)\n",
    "data_loaders[\"train\"].__len__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_x = np.load(\"dump/hfc_men_sr24000/norm/train/in_tacotron/Seikatsu01_A-A__000010-feats.npy\")\n",
    "# out = np.load(\"dump/hfc_men_sr24000/norm/train/out_tacotron/Seikatsu01_A-A__000010-feats.npy\")\n",
    "# spk = np.load(\"data/fine-spk.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(config, device, collate_fn):\n",
    "    \"\"\"Setup for traiining\n",
    "\n",
    "    Args:\n",
    "        config (dict): configuration for training\n",
    "        device (torch.device): device to use for training\n",
    "        collate_fn (callable): function to collate mini-batches\n",
    "\n",
    "    Returns:\n",
    "        (tuple): tuple containing model, optimizer, learning rate scheduler,\n",
    "            data loaders, tensorboard writer, and logger.\n",
    "    \"\"\"\n",
    "    # NOTE: hydra は内部で stream logger を追加するので、二重に追加しないことに注意\n",
    "    logger = getLogger(100, add_stream_handler=False)\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"Random seed: {773}\")\n",
    "    init_seed(773)\n",
    "\n",
    "    # モデルのインスタンス化\n",
    "    model = hydra.utils.instantiate(config.model.netG).to(device)\n",
    "    logger.info(model)\n",
    "    logger.info(\n",
    "        \"Number of trainable params: {:.3f} million\".format(\n",
    "            num_trainable_params(model) / 1000000.0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # (optional) 学習済みモデルの読み込み\n",
    "    # ファインチューニングしたい場合\n",
    "    pretrained_checkpoint = config.train.pretrained.checkpoint\n",
    "    if pretrained_checkpoint is not None and len(pretrained_checkpoint) > 0:\n",
    "        logger.info(\n",
    "            \"Fine-tuning! Loading a checkpoint: {}\".format(pretrained_checkpoint)\n",
    "        )\n",
    "        checkpoint = torch.load(pretrained_checkpoint, map_location=device)\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "        model_dict = model.state_dict()\n",
    "        state_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
    "        invalid_keys = []\n",
    "        for k, v in state_dict.items():\n",
    "            if model_dict[k].shape != v.shape:\n",
    "                logger.info(f\"Skip loading {k}\")\n",
    "                invalid_keys.append(k)\n",
    "        for k in invalid_keys:\n",
    "            state_dict.pop(k)\n",
    "        model_dict.update(state_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "    # 複数 GPU 対応\n",
    "    if config.data_parallel:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer_class = getattr(optim, config.train.optim.optimizer.name)\n",
    "    optimizer = optimizer_class( model.parameters(), **config.train.optim.optimizer.params )\n",
    "\n",
    "    # 学習率スケジューラ\n",
    "    lr_scheduler_class = getattr( optim.lr_scheduler, config.train.optim.lr_scheduler.name )\n",
    "    lr_scheduler = lr_scheduler_class( optimizer, **config.train.optim.lr_scheduler.params )\n",
    "\n",
    "    # DataLoader\n",
    "    data_loaders = get_data_loaders(config.data, collate_fn)\n",
    "\n",
    "    set_epochs_based_on_max_steps_(config.train, len(data_loaders[\"train\"]), logger)\n",
    "\n",
    "    # Tensorboard の設定\n",
    "    writer = SummaryWriter(to_absolute_path(config.train.log_dir))\n",
    "\n",
    "    # config ファイルを保存しておく\n",
    "    out_dir = Path(to_absolute_path(config.train.out_dir))\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_dir / \"model.yaml\", \"w\") as f:\n",
    "        OmegaConf.save(config.model, f)\n",
    "    with open(out_dir / \"config.yaml\", \"w\") as f:\n",
    "        OmegaConf.save(config, f)\n",
    "\n",
    "    return model, optimizer, lr_scheduler, data_loaders, writer, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hydra.main(config_path=\"conf/train_tacotron\", config_name=\"config\")\n",
    "# def my_app(config: DictConfig) -> None:\n",
    "def my_app():\n",
    "    global logger\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    collate_fn = partial(\n",
    "        collate_fn_ms_tacotron, reduction_factor=2\n",
    "    )\n",
    "    model, optimizer, lr_scheduler, data_loaders, writer, logger = setup(\n",
    "        device, collate_fn\n",
    "    )\n",
    "    # train_loop(config, device, model, optimizer, lr_scheduler, data_loaders, writer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/user/My Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb セル 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m my_app()\n",
      "\u001b[1;32m/Users/user/My Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb セル 11\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mmps\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m collate_fn \u001b[39m=\u001b[39m partial(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     collate_fn_ms_tacotron, reduction_factor\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model, optimizer, lr_scheduler, data_loaders, writer, logger \u001b[39m=\u001b[39m setup(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     device, collate_fn\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/user/My Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb セル 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m init_seed(\u001b[39m773\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# モデルのインスタンス化\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model \u001b[39m=\u001b[39m hydra\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39minstantiate(config\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnetG)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mNumber of trainable params: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m million\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         num_trainable_params(model) \u001b[39m/\u001b[39m \u001b[39m1000000.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/My%20Document/Tacotron_Exp/exp_tacotron2/test_retrain.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "my_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra.utils import to_absolute_path\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def load_config():\n",
    "    hydra_instance = GlobalHydra.instance()\n",
    "    if not hydra_instance.is_initialized():\n",
    "        hydra_instance.clear()\n",
    "        initialize(config_path=\"conf/train_tacotron\")\n",
    "    \n",
    "    cfg = compose(config_name=\"config\")\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train.list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gs/nm1_2m6x6dsfq_1hwx0k12nc0000gn/T/ipykernel_73924/1372907968.py:11: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"conf/train_tacotron\")\n",
      "/Users/user/My Document/Tacotron_Exp/.taco_env/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "config = load_config()\n",
    "print(config.data.train.utt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker Idの取り扱い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    \"dump/test/test-spk.npy\",\n",
    "    np.array([10], dtype=np.int64),\n",
    "    allow_pickle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"dump/test/test-spk.npy\", allow_pickle=False)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1],\n",
      "        [2],\n",
      "        [3]]), tensor([[4],\n",
      "        [5],\n",
      "        [6]]))\n",
      "tensor([[[1],\n",
      "         [2],\n",
      "         [3]],\n",
      "\n",
      "        [[4],\n",
      "         [5],\n",
      "         [6]]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[1], [2], [3]])\n",
    "tensor2 = torch.tensor([[4], [5], [6]])\n",
    "tuple1 = (tensor1, tensor2)\n",
    "stack = torch.stack(tuple1, 0)\n",
    "print(tuple1)\n",
    "print(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mask test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6, 1])\n",
      "tensor([[[False],\n",
      "         [False],\n",
      "         [False],\n",
      "         [False],\n",
      "         [False],\n",
      "         [ True]],\n",
      "\n",
      "        [[False],\n",
      "         [False],\n",
      "         [False],\n",
      "         [ True],\n",
      "         [ True],\n",
      "         [ True]],\n",
      "\n",
      "        [[False],\n",
      "         [False],\n",
      "         [False],\n",
      "         [False],\n",
      "         [ True],\n",
      "         [ True]]])\n",
      "tensor([[ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.util import make_non_pad_mask, make_pad_mask\n",
    "\n",
    "lengths = [5, 3, 4]\n",
    "maxlen = 6\n",
    "mask = make_pad_mask(lengths, maxlen).unsqueeze(-1)\n",
    "non_mask = make_non_pad_mask(lengths, maxlen)\n",
    "\n",
    "# 結果を出力\n",
    "print(mask.shape)\n",
    "print(mask)\n",
    "print(non_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "tensor([ 1,  2,  3,  4,  5,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    }
   ],
   "source": [
    "# ダミーのテンソルを作成\n",
    "tensor = torch.tensor([[1, 2, 3, 4, 5, 0], [4, 5, 6, 0, 0, 0], [7, 8, 9, 10, 0, 0]])\n",
    "masked_tensor = tensor.masked_select(mask.squeeze(-1))\n",
    "non_masked_tensor = tensor.masked_select(non_mask)\n",
    "print(masked_tensor)\n",
    "print(non_masked_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### バッチ数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466.375\n",
      "58.28125\n",
      "1865\n"
     ]
    }
   ],
   "source": [
    "utt_ids_train = load_utt_list(to_absolute_path(\"data/train.list\"))\n",
    "utt_ids_dev = load_utt_list(to_absolute_path(\"data/dev.list\"))\n",
    "print(len(utt_ids_train) / 32)\n",
    "print(len(utt_ids_dev) / 32)\n",
    "print(len(utt_ids_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'31.25 per epoch'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print((1000 / 32) * 4)\n",
    "\"31.25 per epoch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test early stopping class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.early_stopping import EarlyStopping\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping! loss value exceed consecutively 6 times\n",
      "flag is : True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "flag = False\n",
    "for epoch in range(10):\n",
    "    for phase in [\"train\", \"dev\"]:\n",
    "        sum_loss = 0\n",
    "        for iter in range(50):\n",
    "            if random.randint(1, 10) < 6:\n",
    "                sum_loss += 500\n",
    "            else:\n",
    "                sum_loss -= 1500\n",
    "        if phase == \"dev\":\n",
    "            loss = sum_loss / 50\n",
    "            flag = early_stopping(loss)\n",
    "    \n",
    "    if flag:\n",
    "        print(\"flag is : True\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".taco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
